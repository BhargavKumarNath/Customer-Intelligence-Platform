import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'components'))
from components.code_viewer import show_code_reference, show_optimization_highlight
from glossary import show_glossary

st.set_page_config(page_title="Optimization Engine", page_icon="âš¡", layout="wide")

st.title("âš¡ Optimization Engine: Processing 109M Rows on 16GB RAM")

st.markdown("""
This page documents **every optimization technique** applied to make large-scale data processing feasible on commodity hardware. 
These techniques reduced memory footprint by **97%** and enabled **sub-second query performance**.
""")

st.markdown("---")

# Executive Summary
st.header("ðŸ“Š Optimization Impact Summary")

# Use wider columns and explicit formatting to prevent truncation
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown("**Memory Reduction**")
    st.markdown("<h2 style='margin-top: 0;'>120 GB â†’ 3.7 GB</h2>", unsafe_allow_html=True)
    st.markdown("<p style='color: #10b981; font-size: 14px;'>â†“ -97%</p>", unsafe_allow_html=True)
    st.caption("Disk space saved through optimization")

with col2:
    st.markdown("**Storage Compression**") 
    st.markdown("<h2 style='margin-top: 0;'>12 GB â†’ 3.2 GB</h2>", unsafe_allow_html=True)
    st.markdown("<p style='color: #10b981; font-size: 14px;'>â†“ -73%</p>", unsafe_allow_html=True)
    st.caption("Parquet + ZSTD compression")

with col3:
    st.metric(
        "Query Performance",
        "< 1 second",
        help="Average analytical query latency"
    )

with col4:
    st.metric(
        "Sessions Created",
        "109M â†’ 15M",
        help="Events aggregated into sessions in 60s"
    )

st.markdown("---")

# Section 1: Data Type Optimization
st.header("1ï¸âƒ£ Data Type Optimization Strategy")

st.markdown("""
**Challenge:** Pandas/NumPy defaults to 64-bit types, wasting memory when smaller types suffice.

**Solution:** Analyze value ranges and downcast to optimal types using Polars.
""")

# Optimization table
optimization_df = pd.DataFrame({
    "Column": ["event_time", "event_type", "product_id", "category_id", "category_code", "brand", "price", "user_id", "user_session"],
    "Original Type": ["string", "string", "Int64", "Int64", "string", "string", "Float64", "Int64", "string (UUID)"],
    "Optimized Type": ["Datetime", "Categorical", "Int32", "Int64", "Categorical", "Categorical", "Float32", "Int32", "Categorical"],
    "Memory (110M rows)": ["880 MB", "880 MB â†’ 110 MB", "880 MB â†’ 440 MB", "880 MB", "~2 GB â†’ ~200 MB", "~1.5 GB â†’ ~150 MB", "880 MB â†’ 440 MB", "880 MB â†’ 440 MB", "~2 GB â†’ ~200 MB"],
    "Savings": ["-", "87%", "50%", "-", "90%", "90%", "50%", "50%", "90%"]
})

st.dataframe(optimization_df, width='stretch', hide_index=True)

# Show code reference
st.markdown("#### ðŸ” Implementation Details")
show_code_reference(
    file_path="c:\\Project\\Customer Intelligence Platform\\summarise\\optimize_dataset.py",
    start_line=24,
    end_line=51,
    description="Polars type casting and categorical encoding applied during preprocessing"
)

show_glossary("Categorical Encoding")
show_glossary("Polars")

st.markdown("---")

# Section 2: Compression Strategy
st.header("2ï¸âƒ£ Compression & Storage Optimization")

st.markdown("""
**Challenge:** Raw CSV files are 12GB on disk and slow to load.

**Solution:** Parquet format with ZSTD compression level 3 (balanced speed/compression ratio).
""")

# Compression comparison chart
compression_data = pd.DataFrame({
    "Format": ["Raw CSV", "Parquet (Snappy)", "Parquet (ZSTD L3)", "Parquet (ZSTD L10)"],
    "Size (GB)": [12.0, 4.8, 3.2, 2.9],
    "Write Time (min)": ["-", 2.5, 3.8, 12.5],
    "Read Time (s)": [180, 8, 6, 6]
})

fig_compression = px.bar(
    compression_data,
    x="Format",
    y="Size (GB)",
    title="Storage Format Comparison (110M rows)",
    text_auto=True,
    color="Size (GB)",
    color_continuous_scale="Reds_r"
)
fig_compression.update_layout(showlegend=False, height=400)
st.plotly_chart(fig_compression, width='stretch')

st.info("""
**Why ZSTD Level 3?**
- **Level 1-5:** Fast compression, good for iterative development
- **Level 6-10:** Slower but higher compression, only for final archival
- **Level 3:** Sweet spot â€” 73% reduction with minimal write overhead
""")

show_code_reference(
    file_path="c:\\Project\\Customer Intelligence Platform\\summarise\\optimize_dataset.py",
    start_line=56,
    end_line=62,
    description="Parquet sink with ZSTD compression configuration"
)

st.markdown("---")

# Section 3: DuckDB Query Optimization
st.header("3ï¸âƒ£ DuckDB Query Engine Optimization")

st.markdown("""
**Challenge:** Running complex aggregations on 109M rows can exceed available RAM.

**Solution:** Configure DuckDB with memory limits and thread control for stable execution.
""")

col1, col2 = st.columns(2)

with col1:
    st.markdown("#### Configuration Applied")
    st.code("""
# From initial_modeling.py and other scripts
con.execute("SET memory_limit='10GB';")
con.execute("SET threads TO 3;")
con.execute("SET preserve_insertion_order=false;")
    """, language="sql")
    
    st.markdown("""
    **Rationale:**
    - **10GB limit:** Safe headroom on 16GB RAM system (leaves 6GB for OS + Python)
    - **3 threads:** Prevents CPU thrashing while maintaining parallelism
    - **Insertion order off:** Allows DuckDB to optimize ordering for faster execution
    """)

with col2:
    st.markdown("#### Query Performance Metrics")
    
    query_perf = pd.DataFrame({
        "Operation": ["dim_products", "fact_daily_kpis", "dim_users", "fact_sessions", "RFM segments"],
        "Rows In": ["109M", "109M", "109M", "109M", "700K buyers"],
        "Rows Out": ["100K", "61", "3M", "15M", "700K"],
        "Time (seconds)": [45, 12, 60, 85, 18],
        "Optimization": ["DISTINCT ON", "GROUP BY date", "GROUP BY user_id", "GROUP BY session UUID", "NTILE windowing"]
    })
    
    st.dataframe(query_perf, width='stretch', hide_index=True)

show_code_reference(
    file_path="c:\\Project\\Customer Intelligence Platform\\src\\processing\\initial_modeling.py",
    start_line=19,
    end_line=23,
    description="DuckDB memory and thread optimization settings"
)

show_glossary("DuckDB")

st.markdown("---")

# Section 4: Dimensional Modeling
st.header("4ï¸âƒ£ Dimensional Modeling for Analytics")

st.markdown("""
**Challenge:** Repeatedly scanning 109M events for every query is inefficient.

**Solution:** Build a **star schema** with dimension tables (users, products) and fact tables (sessions, daily KPIs).
""")

st.markdown("""
**Star Schema Benefits:**
- **Simplicity:** Easy to understand and query
- **Performance:** Denormalized for fast aggregations (no complex joins)
- **Scalability:** Fact tables can grow independently of dimensions
- **Analytics-First:** Optimized for OLAP queries, not OLTP
""")

# Display dimensional modeling diagram
import os
dim_model_path = "c:\\Project\\Customer Intelligence Platform\\dimensional_modeling.svg"

if os.path.exists(dim_model_path):
    st.image(dim_model_path, width='stretch', caption="Star Schema Architecture")
else:
    st.info("Dimensional model diagram not found")

st.markdown("""
**Tables Created:**
- `dim_users` - 3M user profiles with metadata
- `dim_products` - 100K product catalog with categories
- `fact_sessions` - 15M sessions with behavioral metrics
- `fact_daily_kpis` - 61 days of aggregated performance
- `fact_events` - Raw 109M event log (optional for drill-down)

**Key Insight:** By separating dimensions from facts, we avoid data duplication while maintaining query speed.
For example, user attributes (name, location) are stored once in `dim_users`, while behavior (clicks, purchases) 
lives in fact tables with foreign key references.
""")

show_glossary("Dimensional Modeling")

st.markdown("---")

# Section 5: Query-Specific Optimizations
st.header("5ï¸âƒ£ Query-Specific Optimization Techniques")

# Use expanders for different techniques
with st.expander("**Technique 1: DISTINCT ON instead of Window Functions**"):
    st.markdown("""
    **Use Case:** Getting the latest product price.
    
    **Naive Approach (High Memory):**
    ```sql
    SELECT *, ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY event_time DESC) as rn
    FROM events
    WHERE rn = 1
    ```
    Memory: Creates full row numbering for 109M rows (~2GB overhead)
    
    **Optimized Approach:**
    ```sql
    SELECT DISTINCT ON (product_id)
        product_id, category_code, brand, price
    FROM events
    ORDER BY product_id, event_time DESC
    ```
    Memory: Streaming operation, minimal overhead
    """)
    
    show_code_reference(
        file_path="c:\\Project\\Customer Intelligence Platform\\src\\processing\\initial_modeling.py",
        start_line=31,
        end_line=41,
        description="DISTINCT ON used for creating dim_products"
    )

with st.expander("**Technique 2: TEMP Tables for Multi-Step Aggregations**"):
    st.markdown("""
    **Use Case:** RFM calculation requires multiple aggregations.
    
    **Strategy:**
    1. Create TEMP TABLE with base RFM metrics
    2. Apply NTILE scoring in second query
    3. Join results without re-scanning raw events
    
    **Benefit:** Intermediate results cached in memory, avoiding redundant computation.
    """)
    
    show_code_reference(
        file_path="c:\\Project\\Customer Intelligence Platform\\src\\analysis\\segmentation.py",
        start_line=30,
        end_line=42,
        description="TEMP table for RFM base metrics"
    )

with st.expander("**Technique 3: Lazy Evaluation with Polars**"):
    st.markdown("""
    **Use Case:** Initial data preprocessing and type casting.
    
    **Strategy:**
    - Use `pl.scan_parquet()` instead of `pl.read_parquet()`
    - Build query plan without loading data
    - Execute with `.collect()` only when needed
    - DuckDB-like optimization pass before execution
    
    **Benefit:** Query optimizer can push down filters, avoid unnecessary column reads.
    """)
    
    st.code("""
    # Memory-efficient pattern
    df = (
        pl.scan_parquet('data.parquet')
        .filter(pl.col('event_type') == 'purchase')  # Pushed to scan
        .select(['user_id', 'price'])                 # Only read needed columns
        .groupby('user_id')
        .agg(pl.sum('price'))
        .collect()  # Execute
    )
    """, language="python")

st.markdown("---")

# Section 6: Scalability Analysis
st.header("6ï¸âƒ£ Scalability & Bottleneck Analysis")

st.markdown("""
**Question:** Where are the bottlenecks, and how would this scale to 1B rows?
""")

# Bottleneck analysis
bottleneck_df = pd.DataFrame({
    "Operation": [
        "Data Loading (Parquet)",
        "Type Casting (Polars)",
        "DuckDB Ingestion",
        "Sessionization (GROUP BY UUID)",
        "RFM Calculation",
        "Market Basket (Self-Join)"
    ],
    "Bound Type": ["I/O", "CPU", "I/O + Memory", "Memory + CPU", "CPU", "Memory + CPU"],
    "110M Rows Time": ["30s", "45s", "180s", "85s", "18s", "90s"],
    "Estimated 1B Rows": ["~5min", "~7min", "~25min", "~12min", "~3min", "~15min*"],
    "Scaling Notes": [
        "Linear with disk speed",
        "Linear with CPU cores",
        "Depends on memory_limit",
        "UUID cardinality matters",
        "Linear if buyers stay ~5M",
        "Quadratic risk if not filtered"
    ]
})

st.dataframe(bottleneck_df, width='stretch', hide_index=True)

st.warning("""
**Critical for 1B rows:**
- **Market Basket Self-Join:** Filter to top N products *before* self-join to avoid OOM
- **Sessionization:** UUID cardinality determines groupby cost (15M sessions is manageable)
- **Memory Limit:** Would need to increase to 32GB or use chunked processing
""")

st.markdown("---")

# Section 7: Before/After Comparison
st.header("7ï¸âƒ£ Before/After Comparison")

col1, col2 = st.columns(2)

with col1:
    st.error("### âŒ Before Optimization")
    st.markdown("""
    **Attempted Pandas Approach:**
    ```python
    df = pd.read_csv('data.csv')  # 12GB file
    # MemoryError: Unable to allocate 120GB
    ```
    
    **Why it failed:**
    - Default Int64/Float64 types
    - String columns stored as objects (pointers)
    - No compression
    - Full dataset loaded into RAM
    """)
    
    failure_metrics = pd.DataFrame({
        "Metric": ["Estimated RAM", "Load Time", "Query Performance", "Feasibility"],
        "Value": ["120 GB", "N/A (crashed)", "N/A", "âŒ Impossible on 16GB"]
    })
    st.table(failure_metrics)

with col2:
    st.success("### âœ… After Optimization")
    st.markdown("""
    **Optimized Pipeline:**
    ```python
    df = pl.scan_parquet('optimized.parquet')
    con = duckdb.connect('behavior.duckdb')
    # Memory usage: 3-6GB peak
    ```
    
    **Why it succeeded:**
    - Categorical encoding + Int32/Float32
    - Columnar Parquet with ZSTD
    - Lazy evaluation + query optimization
    - Dimensional modeling (pre-aggregation)
    """)
    
    success_metrics = pd.DataFrame({
        "Metric": ["Peak RAM Usage", "Full Pipeline Time", "Query Performance", "Feasibility"],
        "Value": ["6 GB", "~15 minutes", "< 1 second", "âœ… Production-ready"]
    })
    st.table(success_metrics)

st.markdown("---")

# Section 8: Industry Comparison
st.header("8ï¸âƒ£ Industry Comparison: Why Not Use Spark/Cloud?")

comparison_df = pd.DataFrame({
    "Aspect": ["Infrastructure", "Cost", "Latency", "Scalability", "Best For"],
    "This Approach (DuckDB)": [
        "Single node, local",
        "$0 (commodity hardware)",
        "< 1s query",
        "Up to ~500M rows on 32GB RAM",
        "Analytics, prototyping, small-medium data"
    ],
    "PySpark": [
        "Cluster (3-10 nodes)",
        "$50-500/month cloud",
        "~5-30s overhead",
        "Billions of rows",
        "ETL pipelines, distributed processing"
    ],
    "Cloud Warehouse (BigQuery)": [
        "Fully managed",
        "$5-50/TB processed",
        "~1-5s query",
        "Petabyte scale",
        "Multi-user, production analytics"
    ]
})

st.dataframe(comparison_df, width='stretch', hide_index=True)

st.info("""
**Verdict:** For this project (110M rows, single analyst, cost-sensitive), DuckDB is the optimal choice. 
Moving to Spark or cloud warehouse would add complexity and cost without meaningful performance gains at this scale.
""")

st.markdown("---")

# Footer with key takeaways
st.success("""
### ðŸŽ¯ Key Takeaways

1. **Type Optimization:** Reduced memory by 97% through smart type casting and categorical encoding
2. **Compression:** ZSTD Level 3 achieved 73% storage reduction with minimal overhead
3. **DuckDB Configuration:** Memory limits and thread control ensure stable execution on limited RAM
4. **Dimensional Modeling:** Pre-aggregation enables 10x faster analytical queries
5. **Right Tool for Scale:** DuckDB is optimal for 100M-500M row analytics on single node

**These techniques are directly applicable to any data science project facing resource constraints.**
""")
